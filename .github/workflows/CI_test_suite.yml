name: CI Test Suite

on:
  push:
    branches: 
      - "main"
      - "henk/*"
  pull_request:
    branches: [ "main" ]

jobs:

  # SystemTests:
  #   runs-on: ubuntu-latest
  #   steps:
  #   - uses: actions/checkout@v4

  #   - name: Set up Go
  #     uses: actions/setup-go@v5
  #     with:
  #       go-version-file: go.mod

  #   - name: Set up Python
  #     uses: actions/setup-python@v5
  #     with:
  #       python-version: '3.12'

  #   - name: System tests dependencies
  #     run: xargs -a system_test_requirements.txt sudo apt-get install
  #     working-directory: test_suite

  #   - name: Performance tests dependencies
  #     run: pip install -r python_requirements.txt
  #     working-directory: test_suite

  #   - name: Run system tests
  #     id: system-test
  #     run: ./system_tests.sh -c 0
  #     working-directory: test_suite
  #     continue-on-error: true

  #   - name: Upload system test logs
  #     uses: actions/upload-artifact@v4
  #     with:
  #       name: system-test-logs
  #       path: test_suite/system_test_logs/

  #   - name: Fail job if system test failed (for clarity in GitHub UI)
  #     if: ${{ steps.system-test.outcome == 'failure' }}
  #     run: exit 1

  PerformanceTests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.22'

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: System tests dependencies
      run: xargs -a system_test_requirements.txt sudo apt-get install
      working-directory: test_suite

    - name: Performance tests dependencies
      run: pip install -r python_requirements.txt
      working-directory: test_suite

    - name: Run performance test with varying bitrate
      id: system-test
      run: ./system_tests.sh -p -L performance
      working-directory: test_suite
      continue-on-error: true

    - name: Upload performance test graphs
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-graphs
        path: test_suite/system_test_logs/performance/*/*.png

    - name: Upload performance test data
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-data
        path: test_suite/system_test_logs/performance/*/performance_test_data.json

    - name: Upload full performance test logs
      uses: actions/upload-artifact@v4
      with:
        name: performance-test-logs
        path: test_suite/system_test_logs/

    - name: Fail job if performance test failed (for clarity in GitHub UI)
      if: ${{ steps.system-test.outcome == 'failure' }}
      run: exit 1

    - name: Download artifact from target branch head
      if: ${{ github.event_name == 'pull_request' }}
      uses: dawidd6/action-download-artifact@v9
      with: 
        branch: ${{ github.base_ref }}
        name: performance-test-data
        path: ./test_suite
        skip_unpack: true
        workflow_conclusion: ""

    - name: Debug previous commit hash
      if: ${{ github.event_name == 'push' }}
      run: echo ${{ github.event.before }}

    - name: Download artifact from previous commit
      if: ${{ github.event_name == 'push' }}
      uses: dawidd6/action-download-artifact@v9
      with: 
        commit: ${{ github.event.before }}
        name: performance-test-data
        path: ./test_suite
        skip_unpack: true
        workflow_conclusion: ""
    
    - name: Unzip artifact
      working-directory: test_suite
      run: unzip performance-test-data.zip -d previous_performance
    
    - name: Check if artifact was successfully downloaded and extracted
      run: ls test_suite/

    - name: Compare current and artifact performance
      id: performance-comparison
      working-directory: test_suite
      run: |
        python compare_performance.py previous_performance/ performance/
        echo "exit-code=$?" >> $GITHUB_OUTPUT 
      continue-on-error: true

    - name: Report result of comparison in GitHub job summary
      run: |
        case ${{ steps.performance-comparison.outputs.exit-code }} in
          0)
            echo "### ✅ No significant performance change" >> $GITHUB_STEP_SUMMARY
            exit 0
            ;;
          1)
            echo "### ❌ Unable to compare performance, see the output of the 'Compare current and artifact performance' step for details" >> $GITHUB_STEP_SUMMARY
            exit 1
            ;;
          2)
            echo "### 📉 Performance has become worse :(" >> $GITHUB_STEP_SUMMARY
            exit 0
            ;;
          3)
            echo "### 📈 Performance has improved!" >> $GITHUB_STEP_SUMMARY
            exit 1
            ;;
        esac

  # IntegrationTests:
  #   runs-on: ubuntu-latest
  #   steps:
  #   - uses: actions/checkout@v4

  #   - name: Set up Go
  #     uses: actions/setup-go@v5
  #     with:
  #       go-version-file: go.mod

  #   - name: Run integration tests
  #     id: integration-test
  #     run: go test -coverpkg=./... -coverprofile cover.out -v ./...

  #   - name: Create integration test coverage report
  #     run: go tool cover -html cover.out -o cover.html

  #   - name: Upload integration test coverage report
  #     uses: actions/upload-artifact@v4
  #     with:
  #       name: integration-test-coverage
  #       path: cover.html

  #   - name: Fail job if integration test failed (for clarity in GitHub UI)
  #     if: ${{ steps.integration-test.outcome == 'failure' }}
  #     run: exit 1